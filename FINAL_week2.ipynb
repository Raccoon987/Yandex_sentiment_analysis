{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#deprecated: from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#modules for features creation in texts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer => Convert a collection of text documents to a matrix of token counts <br>TfidfTransformer => Transform a count matrix to a normalized tf or tf-idf representation <br> TfidfVectorizer => Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "##### SGDClassifier => Linear classifiers (SVM, logistic regression, a.o.) with SGD training. <br> LinearSVC => Linear Support Vector Classification <br> Pipeline => Pipeline of transforms with a final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CorpusView', '_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__name__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_add', '_get_root', '_init', '_read_para_block', '_read_sent_block', '_read_word_block', '_resolve', 'abspath', 'abspaths', 'categories', 'citation', 'encoding', 'ensure_loaded', 'fileids', 'license', 'open', 'paras', 'raw', 'readme', 'root', 'sents', 'subdir', 'unicode_repr', 'words']\n"
     ]
    }
   ],
   "source": [
    "print dir(mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the happy bastard ' s quick movie review damn that y2k bug . it ' s got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . little do they know the power within . . . going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . we don ' t know why the crew was really out in the middle of nowhere , we don ' t know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don ' t know why donald sutherland is stumbling around drunkenly throughout . here , it ' s just \" hey , let ' s chase these people around with some robots \" . the acting is below average , even from the likes of curtis . you ' re more likely to get a kick out of her work in halloween h20 . sutherland is wasted and baldwin , well , he ' s acting like a baldwin , of course . the real star here are stan winston ' s robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone ' s brain . so , if robots and body parts really turn you on , here ' s your movie . otherwise , it ' s pretty much a sunken ship of a movie .\n"
     ]
    }
   ],
   "source": [
    "negids = mr.fileids('neg')\n",
    "posids = mr.fileids('pos')\n",
    "\n",
    "negfeats = [\" \".join(mr.words(fileids=[f])) for f in negids]\n",
    "posfeats = [\" \".join(mr.words(fileids=[f])) for f in posids]\n",
    "\n",
    "texts = negfeats + posfeats\n",
    "labels = [0] * len(negfeats) + [1] * len(posfeats)\n",
    "\n",
    "print texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "             (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>  mean: 0.836022, std: 0.015309 \n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'>  mean: 0.813511, std: 0.010356 \n"
     ]
    }
   ],
   "source": [
    "for vect in [CountVectorizer, TfidfVectorizer]:\n",
    "    score = cross_val_score(text_classifier(vect(),  LogisticRegression()), texts, labels)\n",
    "    print vect, \" mean: %f, std: %f \" %(score.mean(), score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df=10:  [ 0.81287425  0.83333333  0.84534535]\n",
      "min_df=50:  [ 0.80239521  0.81831832  0.81531532]\n"
     ]
    }
   ],
   "source": [
    "print \"min_df=10: \", cross_val_score(text_classifier(CountVectorizer(min_df=10),  LogisticRegression()), texts, labels)\n",
    "print \"min_df=50: \", cross_val_score(text_classifier(CountVectorizer(min_df=50),  LogisticRegression()), texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> 0.836021650393\n",
      "<class 'sklearn.svm.classes.LinearSVC'> 0.827517637398\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.789009068949\n"
     ]
    }
   ],
   "source": [
    "for cls in [LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "    print cls, cross_val_score(text_classifier(CountVectorizer(),  cls()), texts, labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "stop_words =  nltk.corpus.stopwords.words('english')\n",
    "print type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "print stop_words\n",
    "print len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print CountVectorizer().get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora stop words  0.821511631392\n",
      "sklearn stop words  0.806008103912\n"
     ]
    }
   ],
   "source": [
    "print \"corpora stop words \", cross_val_score(text_classifier(CountVectorizer(stop_words = stop_words, analyzer = 'word'),  \n",
    "                                                             cls()), texts, labels).mean()\n",
    "print \"sklearn stop words \", cross_val_score(text_classifier(CountVectorizer(stop_words = \"english\"),  \n",
    "                                                             cls()), texts, labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word bigram:  [ 0.81137725  0.84684685  0.85285285]\n",
      "character n-gram:  [ 0.80838323  0.81381381  0.79129129]\n"
     ]
    }
   ],
   "source": [
    "print \"word bigram: \", cross_val_score(text_classifier(CountVectorizer(ngram_range=(1, 2)),  \n",
    "                                                       LogisticRegression()), texts, labels)\n",
    "print \"character n-gram: \", cross_val_score(text_classifier(CountVectorizer(analyzer='char_wb', ngram_range=(1, 5)),  \n",
    "                                                            LogisticRegression()), texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u' ',\n",
       " u' b',\n",
       " u' bl',\n",
       " u' blu',\n",
       " u' blue',\n",
       " u' c',\n",
       " u' ca',\n",
       " u' cat',\n",
       " u' cat ',\n",
       " u' d',\n",
       " u' do',\n",
       " u' dog',\n",
       " u' dog ',\n",
       " u' g',\n",
       " u' gr',\n",
       " u' gre',\n",
       " u' gree',\n",
       " u' m',\n",
       " u' mo',\n",
       " u' mou',\n",
       " u' mous',\n",
       " u' t',\n",
       " u' th',\n",
       " u' the',\n",
       " u' the ',\n",
       " u'a',\n",
       " u'at',\n",
       " u'at ',\n",
       " u'b',\n",
       " u'bl',\n",
       " u'blu',\n",
       " u'blue',\n",
       " u'blue ',\n",
       " u'c',\n",
       " u'ca',\n",
       " u'cat',\n",
       " u'cat ',\n",
       " u'd',\n",
       " u'do',\n",
       " u'dog',\n",
       " u'dog ',\n",
       " u'e',\n",
       " u'e ',\n",
       " u'ee',\n",
       " u'een',\n",
       " u'een ',\n",
       " u'en',\n",
       " u'en ',\n",
       " u'g',\n",
       " u'g ',\n",
       " u'gr',\n",
       " u'gre',\n",
       " u'gree',\n",
       " u'green',\n",
       " u'h',\n",
       " u'he',\n",
       " u'he ',\n",
       " u'l',\n",
       " u'lu',\n",
       " u'lue',\n",
       " u'lue ',\n",
       " u'm',\n",
       " u'mo',\n",
       " u'mou',\n",
       " u'mous',\n",
       " u'mouse',\n",
       " u'n',\n",
       " u'n ',\n",
       " u'o',\n",
       " u'og',\n",
       " u'og ',\n",
       " u'ou',\n",
       " u'ous',\n",
       " u'ouse',\n",
       " u'ouse ',\n",
       " u'r',\n",
       " u're',\n",
       " u'ree',\n",
       " u'reen',\n",
       " u'reen ',\n",
       " u's',\n",
       " u'se',\n",
       " u'se ',\n",
       " u't',\n",
       " u't ',\n",
       " u'th',\n",
       " u'the',\n",
       " u'the ',\n",
       " u'u',\n",
       " u'ue',\n",
       " u'ue ',\n",
       " u'us',\n",
       " u'use',\n",
       " u'use ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The blue dog Blue\",\n",
    "    \"Green the green cat\",\n",
    "    \"The green mouse\",\n",
    "]\n",
    "\n",
    "# CountVectorizer character 2-grams with word boundaries\n",
    "vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5), min_df=1) \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
