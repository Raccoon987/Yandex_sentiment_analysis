{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "\n",
    "import nltk.corpus\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from mlxtend.preprocessing import DenseTransformer as DT\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer: CountVectorizer\n",
    "                'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0], 'vectorizer__min_df' : [1, 10, 20, 30],\n",
    "                'vectorizer__ngram_range' : [(1, 1), (1, 2)]\n",
    "            TfidfVectorizer\n",
    "\n",
    "classifier: LogisticRegression(), LinearSVC(), SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "            RidgeClassifier(), \n",
    "            sklearn.naive_bayes import MultinomialNB\n",
    "            sklearn.naive_bayes import BernoulliNB(binarize=0.0)\n",
    "            \n",
    "metrics: precision, recall, f1, roc_auc, all together - metrics.classification_report(), mean            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### get dataset for train and prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\".\\data\\products_sentiment_train.tsv\", names = [\"text\", \"label\"], header = 0, sep=\"\\t\")\n",
    "#train = pd.read_csv(\"products_sentiment_train.tsv\", encoding=\"utf-8\", names = [\"text\", \"label\"], header = 0, sep=\"\\t\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python 3; python 2 - no quoting\n",
    "test = pd.read_csv(\".\\data\\products_sentiment_test_copy.tsv\", header = 0, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "del test[\"Id\"]\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.iloc[[0]])\n",
    "print(train.iloc[0, 0])\n",
    "print(train.iloc[2, 0])\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count amount of reviews with positive label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[train[\"label\"] == 1].sum())  \n",
    "print(type(train[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### classes are unbalanced - add 500 randomly selected repeated negative reviews into train dataset. This will have a positive effect on the quality of the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_test_ind = train[train[\"label\"] == 0]\n",
    "new_train = shuffle(train.append(train.iloc[list(zero_test_ind.index[:500])]), random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zero_test_ind.index[:500])\n",
    "print(train.iloc[list(zero_test_ind.index[:500])].shape)\n",
    "\n",
    "print(new_train.shape)\n",
    "print(new_train.head())\n",
    "print(\" \")\n",
    "print(new_train[new_train[\"label\"] == 0].shape)\n",
    "print(new_train[new_train[\"label\"] == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial dataset: reviews - train[\"text\"], labels - train[\"label\"] <br> new ballanced dataset: reviews - new_train[\"text\"], labels - new_train[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create token_matrix:  (document number, word number)  values of this matrix - how many times word appear in document; and tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words =  nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_counts = CountVectorizer(stop_words=stop_words)\n",
    "cvect = CountVectorizer()\n",
    "token_matrix = cvect.fit_transform(new_train[\"text\"])\n",
    "token_matrix_ = cvect.fit_transform(train[\"text\"])\n",
    "\n",
    "print(token_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "frequency_counts = tfidf_transformer.fit_transform(token_matrix)\n",
    "frequency_counts_ = tfidf_transformer.fit_transform(token_matrix_)\n",
    "\n",
    "print(frequency_counts.shape)\n",
    "print(frequency_counts_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(token_matrix))\n",
    "print(np.shape(new_train[\"label\"]))\n",
    "print(np.shape(token_matrix_))\n",
    "print(np.shape(train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_token_matrix = pd.DataFrame(token_matrix.A, columns=cvect.get_feature_names())\n",
    "pd_token_matrix_ = pd.DataFrame(token_matrix_.A, columns=cvect.get_feature_names())\n",
    "\n",
    "print(pd_token_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(frequency_counts.A, columns=cvect.get_feature_names()).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### functions for best parameters selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, transformer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimator(classifier, parameters_grid, scorer, data, labels):\n",
    "    pipeline = text_classifier(CountVectorizer(), TfidfTransformer(), classifier)\n",
    "    \n",
    "    grid_cv = GridSearchCV(pipeline, parameters_grid, scoring = scorer, cv = 4)\n",
    "    grid_cv.fit(data, labels)\n",
    "    \n",
    "    return grid_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scorer in ['roc_auc', 'accuracy', 'average_precision', 'f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics - accuracy\n",
    "#### select classifier among: <br> linear classifiers -  LogisticRegression, LinearSVC, SGDClassifier, RidgeClassifier <br> Bayes classifiers - MultinomialNB, BernoulliNB, GaussianNB <br> tree classifiers - DecisionTreeClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier <br> gradient boosting from sklearn library and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_shufled_train = train.append(train.ix[list(zero_test_ind.index[:500])])\n",
    "print(\"SGDClassifier\", \" accuracy \", cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), SGDClassifier(random_state=1)), \n",
    "                                not_shufled_train[\"text\"], not_shufled_train[\"label\"], scoring='accuracy').mean())\n",
    "print(\"SGDClassifier shuffled \", \" accuracy \", cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), SGDClassifier(random_state=1)), \n",
    "                                new_train[\"text\"], new_train[\"label\"], scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for k, clf in {\"LogisticRegression\": LogisticRegression, \"LinearSVC\": LinearSVC, \n",
    "               \"SGDClassifier\": SGDClassifier, \"RidgeClassifier\": RidgeClassifier}.items():\n",
    "    \n",
    "    print(\"initial dataset: \")\n",
    "    print(k, \" accuracy \", cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), clf(random_state=1)), \n",
    "                                             train[\"text\"], train[\"label\"], scoring='accuracy').mean())\n",
    "    print(\"ballanced dataset: \")\n",
    "    print(k, \" accuracy \", cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), clf(random_state=1)), \n",
    "                                new_train[\"text\"], new_train[\"label\"], scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for k, clf in {\"MultinomialNB\": MultinomialNB, \"BernoulliNB\": BernoulliNB}.items():\n",
    "    print(\"initial dataset: \")\n",
    "    print(k, ' accuracy ', cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), clf()), \n",
    "                                             train[\"text\"], train[\"label\"], scoring= 'accuracy').mean())\n",
    "    print(\"ballanced dataset: \")\n",
    "    print(k, ' accuracy ', cross_val_score(text_classifier(CountVectorizer(), TfidfTransformer(), clf()), \n",
    "                                             new_train[\"text\"], new_train[\"label\"], scoring= 'accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"initial dataset, GaussianNB classifier accuracy: \", \n",
    "      cross_val_score(Pipeline([(\"vectorizer\", CountVectorizer()), (\"transformer\", TfidfTransformer()),\n",
    "                                ('to_dense', DenseTransformer()), (\"classifier\", GaussianNB())]), \n",
    "                                train[\"text\"], train[\"label\"], scoring='accuracy').mean())\n",
    "print(\"ballanced dataset, GaussianNB classifier accuracy: \", \n",
    "      cross_val_score(Pipeline([(\"vectorizer\", CountVectorizer()), (\"transformer\", TfidfTransformer()),\n",
    "                                ('to_dense', DenseTransformer()), (\"classifier\", GaussianNB())]), \n",
    "                                new_train[\"text\"], new_train[\"label\"], scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ballanced dataset show better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### tune linear classificators parameters. choose the best one and after this -  tune CountVectorizer() parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid_vectorizer = {\n",
    "    'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0],\n",
    "    'vectorizer__min_df' : [1, 10, 20], \n",
    "    'vectorizer__ngram_range' : [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6)],\n",
    "    'vectorizer__stop_words' : [stop_words, None, \"english\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid_lr = {\n",
    "    'classifier__C' : [0.8,   1   , 1.2, 1.4], \n",
    "    'classifier__max_iter' : [60, 80,    100   , 120], \n",
    "    'classifier__solver' : ['lbfgs',    'liblinear'   , 'sag'], \n",
    "}\n",
    "parameters_grid_sgdc = {\n",
    "    'classifier__loss' : [\"log\",    \"hinge\"   , \"modified_huber\"], \n",
    "    'classifier__penalty' :  [\"l1\",    \"l2\"   , \"elasticnet\"], \n",
    "    'classifier__n_iter' : [4,   5,   6, 8, 10], \n",
    "}\n",
    "parameters_grid_lsvc = {\n",
    "    'classifier__loss' : [\"hinge\", \"squared_hinge\"], \n",
    "    'classifier__max_iter' : [400, 500, 600, 800, 1000],\n",
    "    'classifier__tol' : [1e-5, 1e-4, 1e-3], \n",
    "    'classifier__C' : [0.9, 1.0, 1.1, 1.2], \n",
    "}\n",
    "#'classifier__penalty' : [\"l1\", \"l2\"],\n",
    "\n",
    "parameters_grid_rc = {\n",
    "    'classifier__alpha' : [0.6, 0.8, 1, 1.2, 2, 5],\n",
    "    'classifier__normalize' : [True, False], \n",
    "    'classifier__tol' : [0.0001, 0.0005, 0.001, 0.0015, 0.002],\n",
    "    'classifier__solver' : [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = Pipeline(steps = [(\"vectorizer\", CountVectorizer()), (\"transformer\", TfidfTransformer()), (\"classifier\", LinearSVC())])\n",
    "pipeline_.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LR_grid_search = estimator(LogisticRegression(random_state=1), parameters_grid_lr, 'accuracy', new_train[\"text\"], \n",
    "                           new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LogisticRegression accuracy: \")\n",
    "#print LR_grid_search.grid_scores_\n",
    "print(LR_grid_search.best_score_)\n",
    "print(LR_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SGDC_grid_search = estimator(SGDClassifier(random_state=1), parameters_grid_sgdc, 'accuracy', new_train[\"text\"], \n",
    "                             new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SGDClassifier accuracy: \")\n",
    "#print SGDC_grid_search.grid_scores_\n",
    "print(SGDC_grid_search.best_score_)\n",
    "print(SGDC_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LSVC_grid_search_1 = estimator(LinearSVC(random_state=1), parameters_grid_lsvc, 'accuracy', new_train[\"text\"], \n",
    "                               new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LinearSVC accuracy: \")\n",
    "#print LSVC_grid_search.grid_scores_\n",
    "print(LSVC_grid_search_1.best_score_)\n",
    "print(LSVC_grid_search_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "RC_grid_search = estimator(RidgeClassifier(random_state=1), parameters_grid_rc, 'accuracy', new_train[\"text\"], \n",
    "                               new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RidgeClassifier accuracy: \")\n",
    "#print RC_grid_search.grid_scores_\n",
    "print(RC_grid_search.best_score_)\n",
    "print(RC_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best results with parameters by default showed SGDClassifier, RidgeClassifier and LinearSVC classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune parameters for CountVectorizer() for previously selected SGDClassifier, RidgeClassifier Рё LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SGDC_grid_search_ = estimator(SGDClassifier(n_iter=6, loss='log', penalty='l2', random_state=1), \n",
    "                              parameters_grid_vectorizer, 'accuracy', new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SGDClassifier_full accuracy: \")\n",
    "#print SGDC_grid_search_.grid_scores_\n",
    "print(SGDC_grid_search_.best_score_)\n",
    "print(SGDC_grid_search_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "RC_grid_search_ = estimator(RidgeClassifier(tol=0.0001, solver='auto', alpha=0.6, normalize=True, random_state=1), \n",
    "                            parameters_grid_vectorizer, 'accuracy', new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RidgeClassifier_full accuracy: \")\n",
    "#print RC_grid_search_.grid_scores_\n",
    "print(RC_grid_search_.best_score_)\n",
    "print(RC_grid_search_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LSVC_grid_search_ = estimator(LinearSVC(max_iter=600, loss='squared_hinge', random_state=1), \n",
    "                              parameters_grid_vectorizer, 'accuracy', new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LinearSVC_full accuracy: \")\n",
    "#print LSVC_grid_search_.grid_scores_\n",
    "print(LSVC_grid_search_.best_score_)\n",
    "print(LSVC_grid_search_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LSVC_grid_search__1 = estimator(LinearSVC(max_iter=400, loss='squared_hinge', C= 1.1, tol=1e-05, random_state=1), \n",
    "                             parameters_grid_vectorizer, 'accuracy', new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LinearSVC__1full accuracy: \")\n",
    "#print LSVC_grid_search__1.grid_scores_\n",
    "print(LSVC_grid_search__1.best_score_)\n",
    "print(LSVC_grid_search__1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Best linear classifier - LinearSVC(max_iter=600, loss='squared_hinge', C= 1.1, tol=1e-05) with CountVectorizer(min_df=1, ngram_range=(1, 3), max_df=0.85, stop_words=None)   accuracy - 0.86 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion Tree classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_classifiers = [DecisionTreeClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipeline__ = Pipeline(steps = [(\"classifier\", GradientBoostingClassifier())])\n",
    "#pipeline__.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier\n",
    "      ['classifier__min_impurity_split',  'classifier__max_leaf_nodes',  'classifier__max_features', \n",
    "      'classifier__min_samples_split',   'classifier__min_weight_fraction_leaf',    'classifier__splitter',\n",
    "      'classifier__class_weight',   'steps',   'classifier__max_depth',   'classifier__min_samples_leaf',\n",
    "      'classifier__presort',   'classifier',   'classifier__random_state',   'classifier__criterion']\n",
    "#### RandomForestClassifier\n",
    "      ['classifier__bootstrap',   'classifier__min_impurity_split',   'classifier__max_depth',   'classifier__max_features',\n",
    "       'classifier__min_samples_split',   'classifier__min_samples_leaf',   'classifier__oob_score',\n",
    "       'classifier__class_weight',   'classifier__n_estimators',   'classifier__max_leaf_nodes',   'classifier__random_state',\n",
    "       'steps',   'classifier__warm_start',   'classifier__n_jobs',   'classifier__min_weight_fraction_leaf',   'classifier',\n",
    "       'classifier__verbose',   'classifier__criterion']  \n",
    "#### BaggingClassifier\n",
    "      ['classifier__bootstrap',   'classifier__max_features',   'classifier__base_estimator',   'classifier__oob_score',\n",
    "       'classifier__n_estimators',   'classifier__random_state',   'classifier__max_samples', 'classifier__bootstrap_features',\n",
    "       'steps',   'classifier__warm_start',   'classifier__n_jobs',   'classifier',   'classifier__verbose']       \n",
    "#### GradientBoostingClassifier\n",
    "      ['classifier__min_impurity_split',   'classifier__max_features',   'classifier__subsample',   'classifier__max_depth',\n",
    "       'classifier__alpha',   'classifier__min_samples_split',   'classifier__learning_rate',   'classifier__min_samples_leaf',\n",
    "       'classifier__criterion',   'classifier__loss',   'classifier__n_estimators',   'classifier__max_leaf_nodes',\n",
    "       'steps',   'classifier__warm_start',   'classifier__verbose',   'classifier__presort',   \n",
    "       'classifier__min_weight_fraction_leaf',   'classifier',   'classifier__random_state',   'classifier__init']               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid_dtc = {\n",
    "    'classifier__max_depth' : range(6, 31, 4),\n",
    "    'classifier__min_samples_split' : range(2, 4, 1),\n",
    "    'classifier__min_samples_leaf' : [1, 2, 3],\n",
    "    'classifier__min_impurity_split' : [10 ** (-10), 10 ** (-9), 10 ** (-8), 10 ** (-7)],\n",
    "    \n",
    "}\n",
    "\n",
    "parameters_grid_rfc = {\n",
    "    'classifier__n_estimators' : range(2, 32, 5),\n",
    "    'classifier__max_depth' : [None] + list(range(2, 22, 4)),\n",
    "    'classifier__min_samples_split' : range(2, 4, 1),\n",
    "    'classifier__min_samples_leaf' : [1, 2, 3],\n",
    "    'classifier__max_features' : [\"auto\", \"sqrt\", \"log2\"],\n",
    "    'classifier__min_impurity_split' : [10 ** (-8), 10 ** (-7)],\n",
    "}\n",
    "\n",
    "parameters_grid_bc = {\n",
    "    'classifier__n_estimators' : range(10, 100, 20),\n",
    "    'classifier__warm_start' : [False, True], \n",
    "    'classifier__bootstrap_features' : [False, True], \n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "DTC_grid_search = estimator(DecisionTreeClassifier(random_state=1), parameters_grid_dtc, 'accuracy', \n",
    "                            new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DecisionTreeClassifier accuracy: \")\n",
    "#print DTC_grid_search.grid_scores_\n",
    "print(DTC_grid_search.best_score_)\n",
    "print(DTC_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "RFC_grid_search = estimator(RandomForestClassifier(random_state=1), parameters_grid_rfc, 'accuracy', \n",
    "                            new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RandomForestClassifier accuracy: \")\n",
    "#print RFC_grid_search.grid_scores_\n",
    "print(RFC_grid_search.best_score_)\n",
    "print(RFC_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(\n",
    "                      text_classifier(CountVectorizer(), \n",
    "                                      TfidfTransformer(), \n",
    "                                      RandomForestClassifier(n_estimators=32, min_samples_split=3, max_features='log2', \n",
    "                                                             min_impurity_split=1e-08, random_state=1)), \n",
    "                      new_train[\"text\"], new_train[\"label\"], scoring= 'accuracy', cv=4).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "BC_grid_search = estimator(BaggingClassifier(random_state=1), parameters_grid_bc, 'accuracy', \n",
    "                           new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BaggingClassifier accuracy: \")\n",
    "#print BC_grid_search.grid_scores_\n",
    "print(BC_grid_search.best_score_)\n",
    "print(BC_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best decision tree classifier - RandomForestClassifier Рё BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tune parameters for CountVectorizer() for previously selected RandomForestClassifier Рё BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "RFC_grid_search_ = estimator(RandomForestClassifier(min_impurity_split=1e-08, max_features='log2', min_samples_split=3, \n",
    "                                                    n_estimators=27, min_samples_leaf=1, max_depth=None, random_state=1), \n",
    "                             parameters_grid_vectorizer, 'accuracy', new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RandomForestClassifier full accuracy: \")\n",
    "#print RFC_grid_search_.grid_scores_\n",
    "print(RFC_grid_search_.best_score_)\n",
    "print(RFC_grid_search_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "BC_grid_search_ = estimator(BaggingClassifier(bootstrap_features=True, n_estimators=50, warm_start=False, random_state=1), \n",
    "                            parameters_grid_vectorizer, 'accuracy', new_train[\"text\"], new_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BaggingClassifier full accuracy: \")\n",
    "#print BC_grid_search_.grid_scores_\n",
    "print(BC_grid_search_.best_score_)\n",
    "print(BC_grid_search_.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### tuning CountVectorizer() parameters led to an increase of  BaggingClassifier score and decrease of RandomForestClassifier score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best decision tree classifier - RandomForestClassifier(n_estimators=32, min_samples_split=3,  max_features='log2', min_impurity_split=1e-08, random_state=1); accuracy - 0.8467; CountVectorizer() with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conda install -c rasbt mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline__ = Pipeline(steps = [(\"classifier\", GradientBoostingClassifier())])\n",
    "pipeline__.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial values: <br> 1)min_samples_split = 25 :  ~0.5-1% from total value <br> 2)min_samples_leaf = 50 <br> 3)max_depth = 6 <br> 4)max_features = \"sqrt\" <br> 5)subsample = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {'classifier__n_estimators':range(20,120,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_1 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()), \n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, min_samples_split=25,\n",
    "                                                                                  min_samples_leaf=50, max_depth=6,\n",
    "                                                                                  max_features='sqrt', subsample=0.8, \n",
    "                                                                                  random_state=1))]),\n",
    "                              param_grid = param_test1,  scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_1.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all scores: \", grid_cv_gbr_1.grid_scores_)\n",
    "print(\"best score: \", grid_cv_gbr_1.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test2 = {'classifier__max_depth':range(4,20,4), 'classifier__min_samples_split':range(10,50,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_2 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()), \n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators= 80,\n",
    "                                                                                  max_features='sqrt',subsample=0.8,\n",
    "                                                                                  random_state=1))]),\n",
    "                              param_grid = param_test2, n_jobs=4, scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_2.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all scores: \", grid_cv_gbr_2.grid_scores_)\n",
    "print(\"best score: \", grid_cv_gbr_2.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test3 = {'classifier__max_features':range(390,415,5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_3 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()), \n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators= 80 ,\n",
    "                                                                                  max_depth=16, min_samples_split=30,\n",
    "                                                                                  subsample=0.8,random_state=1))]),\n",
    "                              param_grid = param_test3, n_jobs=4, scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_3.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all scores: \", grid_cv_gbr_3.grid_scores_)\n",
    "print(\"best score: \", grid_cv_gbr_3.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_test4 = {'classifier__subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_4 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()), \n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators= 80 ,\n",
    "                                                                                  max_depth=16, min_samples_split=30,\n",
    "                                                                                  max_features=390, random_state=1))]),\n",
    "                              param_grid = param_test4, n_jobs=4, scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_4.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all scores: \", grid_cv_gbr_4.grid_scores_)\n",
    "print(\"best score: \", grid_cv_gbr_4.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_4.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"GradientBoostingClassifier learning_rate=0.05, n_estimators= 160\", ' accuracy ', \n",
    "      cross_val_score(Pipeline( [(\"vectorizer\", CountVectorizer()), \n",
    "                                 (\"transformer\", TfidfTransformer()),\n",
    "                                 ('to_dense', DT()), \n",
    "                                 (\"classifier\", GradientBoostingClassifier(learning_rate=0.05, n_estimators= 160 ,\n",
    "                                                                           max_depth=16, min_samples_split=30,max_features=390,\n",
    "                                                                           subsample=0.8, random_state=1))] ), \n",
    "                      new_train[\"text\"], new_train[\"label\"], scoring= 'accuracy', cv = 4).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best parameters set at this moment - GradientBoostingClassifier(learning_rate=0.1, n_estimators= 80, max_depth=16, min_samples_split=30, max_features=390, subsample=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start to tune vectorizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_5 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()),\n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators=80 ,\n",
    "                                                                                  subsample=0.8, max_depth=16, \n",
    "                                                                                  min_samples_split=30,max_features=390, \n",
    "                                                                                  random_state=1))]),\n",
    "                               param_grid =  {'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0]}, scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_5.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all scores: \", grid_cv_gbr_5.grid_scores_)\n",
    "print(\"best score: \", grid_cv_gbr_5.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_5.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### same result for max_df = 0.85, 0.9, 0.95 Рё 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_6 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()),\n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators=80 ,\n",
    "                                                                                  subsample=0.8, max_depth=16, \n",
    "                                                                                  min_samples_split=30, max_features='sqrt', \n",
    "                                                                                  random_state=1))]),\n",
    "                              param_grid =  {'vectorizer__min_df' : [1, 10, 20]}, scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_6.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all scores: \", grid_cv_gbr_6.grid_scores_)\n",
    "print(\"best score: \", grid_cv_gbr_6.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_6.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_7 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()),\n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators=80 ,\n",
    "                                                                                  subsample=0.8, max_depth=16, \n",
    "                                                                                  min_samples_split=30, max_features='sqrt', \n",
    "                                                                                  random_state=1))]),\n",
    "                              param_grid =  {'vectorizer__ngram_range' : [(1, 1), (1, 2), (1, 3)]}, \n",
    "                              scoring = 'accuracy', cv = 4, verbose=1)\n",
    "grid_cv_gbr_7.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print grid_cv_gbr_7.grid_scores_\n",
    "print(\"best score: \", grid_cv_gbr_7.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_7.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_cv_gbr_8 = GridSearchCV( Pipeline([(\"vectorizer\", CountVectorizer()),\n",
    "                                        (\"transformer\", TfidfTransformer()),\n",
    "                                        ('to_dense', DT()), \n",
    "                                        (\"classifier\", GradientBoostingClassifier(learning_rate=0.1, n_estimators=80 ,\n",
    "                                                                                  subsample=0.8, max_depth=16, \n",
    "                                                                                  min_samples_split=30, max_features='sqrt', \n",
    "                                                                                  random_state=1))]),\n",
    "                              param_grid =  {'vectorizer__stop_words' : [stop_words, None, \"english\"]}, \n",
    "                              scoring = 'accuracy', cv = 4)\n",
    "grid_cv_gbr_8.fit(np.array(new_train[\"text\"]), np.array(new_train[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print grid_cv_gbr_8.grid_scores_\n",
    "print(\"best score: \", grid_cv_gbr_8.best_score_)\n",
    "print(\"best params: \", grid_cv_gbr_8.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best result - GradientBoostingClassifier(learning_rate=0.1, n_estimators= 80, max_depth=16, min_samples_split=30, max_features=390, subsample=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mingw_path = 'C:\\\\mingw64\\\\bin'\n",
    "#mingw_path1 ='C:\\\\mingw64\\\\x86_64-w64-mingw32\\\\bin'\n",
    "#os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid_xgbc = {'classifier__n_estimators' : [1] + list(range(55, 200, 10))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_ = Pipeline(steps = [(\"vectorizer\", CountVectorizer()), \n",
    "                              (\"transformer\", TfidfTransformer()), \n",
    "                              (\"classifier\", xgb.XGBClassifier())])\n",
    "pipeline_.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_scoring = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_trees = list(range(1, 250, 25)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scoring = {}\n",
    "for n_tree in n_trees:\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=n_tree, min_child_weight=3, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy', n_jobs=-1, cv = 4).mean()    \n",
    "    xgb_scoring[n_tree] = score\n",
    "xgb_scoring = np.asmatrix(xgb_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[{176: 0.7611176271811256, 1: 0.6310629618251823, 226: 0.7615131502416645, 51: 0.7319022036536413, 101: 0.7455079913164577, 151: 0.7591169820594741, 201: 0.7615157102482183, 26: 0.6998893462767264, 76: 0.7427099021053494, 126: 0.7551080117965103}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best n_estimators: 176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tune max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_depth = range(4, 24, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_scoring_1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scoring_1 = {}\n",
    "for depth in max_depth:\n",
    "    print(depth, end=\" \")\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=depth, n_estimators=175, min_child_weight=3, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy', n_jobs=-1, cv = 4).mean()    \n",
    "    xgb_scoring_1[depth] = score\n",
    "xgb_scoring_1 = np.asmatrix(xgb_scoring_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[{16: 0.7923228106823954, 18: 0.7987285737691487, 4: 0.7579144220529204, 6: 0.7635182784467928, 8: 0.767916365609896, 10: 0.7667202076677316, 12: 0.7763221409846809, 14: 0.7835202404358156}, {20: 0.7959253747849595, 22: 0.7943260055705742}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best max_depth: 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tune min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_child_weight = range(1,9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_scoring_2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for weight in min_child_weight:\n",
    "    print(weight, end=\" \")\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=18, n_estimators=175, min_child_weight=weight, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy',n_jobs=-1 , cv = 4).mean()    \n",
    "    xgb_scoring_2[weight] = score\n",
    "xgb_scoring_2 = np.asmatrix(xgb_scoring_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[{1: 0.8251414250020479, 3: 0.7987285737691487, 5: 0.7667208425493569, 7: 0.7487124866879659}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best min_child_weight: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tune subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsample = [s/10.0 for s in range(6, 10)]\n",
    "xgb_scoring_3 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in subsample:\n",
    "    print(i, end=\" \")\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=18, n_estimators=175, min_child_weight=1, subsample=i, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy', n_jobs=-1 , cv = 4).mean()    \n",
    "    xgb_scoring_3[i] = score\n",
    "xgb_scoring_3 = np.asmatrix(xgb_scoring_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[{0.6: 0.8187369460145818, 0.7: 0.8147356578192839, 0.9: 0.8203382239698533, 0.8: 0.8207375768001967}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best subsample: default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### tune gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_scoring_4 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = [g/10.0 for g in range(0, 5)]\n",
    "xgb_scoring_4 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in gamma:\n",
    "    print(i, end=\" \")\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=18, n_estimators=175, min_child_weight=1, gamma=i, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy', n_jobs=-1 , cv = 4).mean()    \n",
    "    xgb_scoring_4[i] = score\n",
    "xgb_scoring_4 = np.asmatrix(xgb_scoring_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[{0.0: 0.8251414250020479, 0.1: 0.8203426947652985, 0.2: 0.8239375829442124, 0.4: 0.8227350311296797, 0.3: 0.8255407880724175}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### best gamma: 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tune colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collsample = [k/10.0 for k in range(6, 10)]\n",
    "xgb_scoring_5 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in collsample:\n",
    "    print(i, end=\" \")\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=18, n_estimators=175, min_child_weight=1, colsample_bytree=i, \n",
    "                                  gamma=0.3, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy', n_jobs=-1 , cv = 4).mean()    \n",
    "    xgb_scoring_5[i] = score\n",
    "xgb_scoring_5 = np.asmatrix(xgb_scoring_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[{0.6: 0.8215337367903662, 0.7: 0.8207433439829606, 0.9: 0.8207446239862375, 0.8: 0.8167414127140165}]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best colsample_bytree: default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tune regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = [1e-5, 1e-4, 1e-2, 1, 10]\n",
    "xgb_scoring_6 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in reg:\n",
    "    print(i, end=\" \")\n",
    "    estimator = xgb.XGBClassifier(learning_rate=0.1, max_depth=18, n_estimators=175, min_child_weight=1, gamma=0.3, \n",
    "                                  reg_alpha=i, seed=0)\n",
    "    score = cross_val_score(estimator, pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), \n",
    "                            scoring = 'accuracy', n_jobs=-1 , cv = 4).mean()    \n",
    "    xgb_scoring_6[i] = score\n",
    "xgb_scoring_6 = np.asmatrix(xgb_scoring_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_scoring_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### best regularization: 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final best xgboost parameters: xgb.XGBClassifier(learning_rate=0.1, max_depth=18, n_estimators=175, min_child_weight=1, gamma=0.3, reg_alpha=0,0001, seed=0); <br> final score: 0.8287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print cross_val_score(xgb.XGBClassifier(learning_rate=0.2, max_depth=16, n_estimators=200, min_child_weight=3, seed=0), \n",
    "                      pd_token_matrix.as_matrix(), np.array(new_train[\"label\"]), scoring = 'accuracy', n_jobs=-1, cv = 4).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best linear classifier: <br>  LinearSVC(max_iter=600, loss='squared_hinge') with CountVectorizer(min_df=1, ngram_range=(1, 3), max_df=0.85, stop_words=None); <br><br> Best tree classifier: <br>  RandomForestClassifier(n_estimators=32, min_samples_split=3, max_features='log2', min_impurity_split=1e-08, random_state=1) and GradientBoostingClassifier(learning_rate=0.05, n_estimators= 160 , max_depth=16,min_samples_split=30,max_features=390, subsample=0.8, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[\"text\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lin_SVC = text_classifier(vectorizer=CountVectorizer(min_df=1, ngram_range=(1, 5), max_df=0.85, stop_words=None), \n",
    "                transformer=TfidfTransformer(), \n",
    "                classifier=LinearSVC(max_iter=400, loss='squared_hinge', C= 1.1, tol=1e-05, random_state=1))\n",
    "\n",
    "Lin_SVC.fit(new_train[\"text\"], new_train[\"label\"])\n",
    "print(Lin_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_svc_result = Lin_SVC.predict(test[\"text\"])\n",
    "print(lin_svc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"answer_lin_csv_4.csv\", 'w') as f_out:\n",
    "        f_out.write(pd.DataFrame(pd.Series(map(str, range(0, 500))).str.cat(map(str, lin_svc_result), sep=','), \n",
    "                                 columns = [\"Id,y\"]).to_csv(sep=\" \", index=False))\n",
    "\n",
    "#pd.DataFrame(lin_svc_result).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_class = text_classifier(vectorizer=CountVectorizer(), \n",
    "                transformer=TfidfTransformer(), \n",
    "                classifier=RandomForestClassifier(n_estimators=32, min_samples_split=3, max_features='log2', \n",
    "                                                  min_impurity_split=1e-08, random_state=1))\n",
    "\n",
    "rf_class.fit(new_train[\"text\"], new_train[\"label\"])\n",
    "print rf_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result = rf_class.predict(test[\"text\"])\n",
    "print rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"answer_rf_1.csv\", 'w') as f_out:\n",
    "        f_out.write(pd.DataFrame(pd.Series(map(str, range(0, 500))).str.cat(map(str, rf_result), sep=','), \n",
    "                                 columns = [\"Id,y\"]).to_csv(sep=\" \", index=False))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_example = pd.read_csv(\"products_sentiment_sample_submission.csv\", header = 0, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(min_df=1, ngram_range=(1, 3), max_df=0.85, stop_words=None), \n",
    "                transformer=TfidfTransformer(), \n",
    "                classifier=LinearSVC(max_iter=400, loss='squared_hinge', C= 1.1, tol=1e-05, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./data/result.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### it may be possible try to improve result using neural networks or try to somehow transforme features - like reduce space dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dimensionaly reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = PCA(n_components=2500, svd_solver='full')\n",
    "model.fit(pd.DataFrame(frequency_counts.A, columns=cvect.get_feature_names()))\n",
    "reduced_token_mtx = model.transform(pd.DataFrame(frequency_counts.A, columns=cvect.get_feature_names()))\n",
    "print(reduced_token_mtx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvect_r = CountVectorizer(ngram_range=(1, 2))\n",
    "token_mtx_r = cvect_r.fit_transform(new_train[\"text\"])\n",
    "\n",
    "tfidf_r = TfidfTransformer()\n",
    "freq_r = tfidf_r.fit_transform(token_mtx_r)\n",
    "\n",
    "model_ = PCA(n_components=2500, svd_solver='full')\n",
    "model_.fit(pd.DataFrame(freq_r.A, columns=cvect_r.get_feature_names()))\n",
    "reduced_pd = model_.transform(reduce_pd_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cross_val_score(RandomForestClassifier(n_estimators=36, min_samples_split=3, \n",
    "                                             max_features='log2', min_impurity_split=1e-08, \n",
    "                                             random_state=1), \n",
    "                      reduced_pd, new_train[\"label\"], scoring= 'accuracy', cv=4).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
