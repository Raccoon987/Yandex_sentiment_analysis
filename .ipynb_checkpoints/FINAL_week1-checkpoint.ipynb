{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# modules for feature creation on texts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer => Convert a collection of text documents to a matrix of token counts <br> TfidfTransformer => Transform a count matrix to a normalized tf or tf-idf representation <br> TfidfVectorizer => Convert a collection of raw documents to a matrix of TF-IDF features. Equivalent to CountVectorizer followed by TfidfTransformer <br> SGDClassifier => Linear classifiers (SVM, logistic regression, a.o.) with SGD training <br> LinearSVC => Linear Support Vector Classification <br> Pipeline => Pipeline of transforms with a final estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedPlaintextCorpusReader in u'.../corpora/movie_reviews' (not loaded yet)>\n",
      "\n",
      "    To see the API documentation for this lazily loaded corpus, first\n",
      "    run corpus.ensure_loaded(), and then run help(this_corpus).\n",
      "    \n",
      "    LazyCorpusLoader is a proxy object which is used to stand in for a\n",
      "    corpus object before the corpus is loaded.  This allows NLTK to\n",
      "    create an object for each corpus, but defer the costs associated\n",
      "    with loading those corpora until the first time that they're\n",
      "    actually accessed.\n",
      "\n",
      "    The first time this object is accessed in any way, it will load\n",
      "    the corresponding corpus, and transform itself into that corpus\n",
      "    (by modifying its own ``__class__`` and ``__dict__`` attributes).\n",
      "\n",
      "    If the corpus can not be found, then accessing this object will\n",
      "    raise an exception, displaying installation instructions for the\n",
      "    NLTK data package.  Once they've properly installed the data\n",
      "    package (or modified ``nltk.data.path`` to point to its location),\n",
      "    they can then use the corpus object without restarting python.\n",
      "    \n",
      "    :param name: The name of the corpus\n",
      "    :type name: str\n",
      "    :param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n",
      "    :type reader: nltk.corpus.reader.api.CorpusReader\n",
      "    :param nltk_data_subdir: The subdirectory where the corpus is stored.\n",
      "    :type nltk_data_subdir: str\n",
      "    :param *args: Any other non-keywords arguments that `reader_cls` might need.\n",
      "    :param *kargs: Any other keywords arguments that `reader_cls` might need.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(mr)\n",
    "print(mr.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CorpusView', '_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__name__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_add', '_get_root', '_init', '_read_para_block', '_read_sent_block', '_read_word_block', '_resolve', 'abspath', 'abspaths', 'categories', 'citation', 'encoding', 'ensure_loaded', 'fileids', 'license', 'open', 'paras', 'raw', 'readme', 'root', 'sents', 'subdir', 'unicode_repr', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(mr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first way to get positive and negative reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt', 'pos/cv005_29443.txt', 'pos/cv006_15448.txt', 'pos/cv007_4968.txt', 'pos/cv008_29435.txt', 'pos/cv009_29592.txt']\n",
      " \n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n"
     ]
    }
   ],
   "source": [
    "documents = defaultdict(list)\n",
    "\n",
    "for i in mr.fileids():\n",
    "    documents[i.split('/')[0]].append(i)\n",
    "\n",
    "print(documents['pos'][:10]) # first ten pos reviews.\n",
    "print(\" \")\n",
    "print(documents['neg'][:10]) # first ten neg reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the happy bastard ' s quick movie review damn that y2k bug . it ' s got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . little do they know the power within . . . going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . we don ' t know why the crew was really out in the middle of nowhere , we don ' t know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don ' t know why donald sutherland is stumbling around drunkenly throughout . here , it ' s just \" hey , let ' s chase these people around with some robots \" . the acting is below average , even from the likes of curtis . you ' re more likely to get a kick out of her work in halloween h20 . sutherland is wasted and baldwin , well , he ' s acting like a baldwin , of course . the real star here are stan winston ' s robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone ' s brain . so , if robots and body parts really turn you on , here ' s your movie . otherwise , it ' s pretty much a sunken ship of a movie .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(mr.words(fileids=[documents['neg'][1]])), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### second way to get positive and negative reviews and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the happy bastard ' s quick movie review damn that y2k bug . it ' s got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . little do they know the power within . . . going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . we don ' t know why the crew was really out in the middle of nowhere , we don ' t know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don ' t know why donald sutherland is stumbling around drunkenly throughout . here , it ' s just \" hey , let ' s chase these people around with some robots \" . the acting is below average , even from the likes of curtis . you ' re more likely to get a kick out of her work in halloween h20 . sutherland is wasted and baldwin , well , he ' s acting like a baldwin , of course . the real star here are stan winston ' s robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone ' s brain . so , if robots and body parts really turn you on , here ' s your movie . otherwise , it ' s pretty much a sunken ship of a movie .\n"
     ]
    }
   ],
   "source": [
    "negids = mr.fileids('neg')\n",
    "posids = mr.fileids('pos')\n",
    "\n",
    "negfeats = [\" \".join(mr.words(fileids=[f])) for f in negids]\n",
    "posfeats = [\" \".join(mr.words(fileids=[f])) for f in posids]\n",
    "\n",
    "texts = negfeats + posfeats\n",
    "labels = [0] * len(negfeats) + [1] * len(posfeats)\n",
    "\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of reviews:  2000\n",
      "fraction of class 1 in dataset:  0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"total amount of reviews: \", len(labels))\n",
    "print(\"fraction of class 1 in dataset: \", float(len(posfeats))/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counts = CountVectorizer()\n",
    "token_matrix = token_counts.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to select parameters for CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer__ngram_range',\n",
       " 'classifier__max_iter',\n",
       " 'vectorizer__max_features',\n",
       " 'classifier__class_weight',\n",
       " 'vectorizer__max_df',\n",
       " 'classifier__verbose',\n",
       " 'vectorizer__encoding',\n",
       " 'classifier__C',\n",
       " 'classifier__multi_class',\n",
       " 'classifier__intercept_scaling',\n",
       " 'vectorizer__input',\n",
       " 'classifier__warm_start',\n",
       " 'vectorizer__preprocessor',\n",
       " 'vectorizer',\n",
       " 'vectorizer__min_df',\n",
       " 'vectorizer__token_pattern',\n",
       " 'vectorizer__analyzer',\n",
       " 'vectorizer__binary',\n",
       " 'vectorizer__lowercase',\n",
       " 'vectorizer__tokenizer',\n",
       " 'classifier__fit_intercept',\n",
       " 'classifier__solver',\n",
       " 'vectorizer__stop_words',\n",
       " 'classifier__n_jobs',\n",
       " 'classifier__dual',\n",
       " 'vectorizer__vocabulary',\n",
       " 'vectorizer__dtype',\n",
       " 'classifier__tol',\n",
       " 'vectorizer__decode_error',\n",
       " 'steps',\n",
       " 'vectorizer__strip_accents',\n",
       " 'classifier',\n",
       " 'classifier__random_state',\n",
       " 'classifier__penalty']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_ = Pipeline(steps = [(\"vectorizer\", CountVectorizer()), (\"classifier\", LogisticRegression())])\n",
    "pipeline_.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select parameters from: 'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0],     'vectorizer__min_df' : [1, 10, 20, 30],      'vectorizer__ngram_range' : [(1, 1), (1, 2)]\n",
    "#### vectorizer__max_df - if word appears more than in 85, 90, 95, 100% of documents - discard this word; vectorizer__min_df - if word appears less often than in 1, 10, 20, 30 documents - discard this word; vectorizer__ngram_range - build dictionary using single words or involving bigrames\n",
    "\n",
    "#### scoring = 'accuracy', 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'vectorizer__max_df' : [0.85, 0.9, 0.95, 1.0],\n",
    "    'vectorizer__min_df' : [1, 10, 20, 30], \n",
    "    'vectorizer__ngram_range' : [(1, 1), (1, 2)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "    ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'vectorizer__min_df': [1, 10, 20, 30], 'vectorizer__ngram_range': [(1, 1), (1, 2)], 'vectorizer__max_df': [0.85, 0.9, 0.95, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_cv = GridSearchCV(pipeline_, parameters_grid, scoring = 'accuracy', cv = 4)\n",
    "grid_cv.fit(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849\n",
      "{'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 2), 'vectorizer__max_df': 0.85}\n"
     ]
    }
   ],
   "source": [
    "print(grid_cv.best_score_)\n",
    "print(grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv_ = GridSearchCV(pipeline_, parameters_grid, scoring = 'roc_auc', cv = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "    ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'vectorizer__min_df': [1, 10, 20, 30], 'vectorizer__ngram_range': [(1, 1), (1, 2)], 'vectorizer__max_df': [0.85, 0.9, 0.95, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_cv_.fit(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924748\n",
      "{'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 2), 'vectorizer__max_df': 0.85}\n"
     ]
    }
   ],
   "source": [
    "print(grid_cv_.best_score_)\n",
    "print(grid_cv_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transforme reviews into features matrix (num of document x num of word) filled with numbers that represents how many times this word occurs in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 11097)\t1\n",
      "  (0, 33630)\t1\n",
      "  (0, 30354)\t1\n",
      "  (0, 8357)\t2\n",
      "  (0, 39010)\t1\n",
      "  (0, 3991)\t1\n",
      "  (0, 9)\t10\n",
      "  (0, 33806)\t1\n",
      "  (0, 11391)\t1\n",
      "  (0, 23841)\t1\n",
      "  (0, 18950)\t1\n",
      "  (0, 38699)\t1\n",
      "  (0, 32114)\t1\n",
      "  (0, 38679)\t1\n",
      "  (0, 12146)\t1\n",
      "  (0, 31519)\t1\n",
      "  (0, 32014)\t1\n",
      "  (0, 1311)\t1\n",
      "  (0, 39396)\t1\n",
      "  (0, 27310)\t1\n",
      "  (0, 39220)\t1\n",
      "  (0, 1579)\t1\n",
      "  (0, 19446)\t1\n",
      "  (0, 16870)\t1\n",
      "  (0, 14554)\t1\n",
      "  :\t:\n",
      "  (0, 14740)\t1\n",
      "  (0, 16534)\t1\n",
      "  (0, 5187)\t10\n",
      "  (0, 9724)\t1\n",
      "  (0, 15609)\t1\n",
      "  (0, 35280)\t38\n",
      "  (0, 24386)\t16\n",
      "  (0, 24508)\t3\n",
      "  (0, 844)\t1\n",
      "  (0, 1760)\t3\n",
      "  (0, 18386)\t5\n",
      "  (0, 14630)\t3\n",
      "  (0, 35351)\t5\n",
      "  (0, 10748)\t1\n",
      "  (0, 35305)\t3\n",
      "  (0, 1810)\t20\n",
      "  (0, 10737)\t1\n",
      "  (0, 25442)\t1\n",
      "  (0, 6402)\t1\n",
      "  (0, 35714)\t16\n",
      "  (0, 14902)\t2\n",
      "  (0, 8017)\t1\n",
      "  (0, 35033)\t4\n",
      "  (0, 36577)\t2\n",
      "  (0, 26455)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(token_matrix[0]))\n",
    "print(token_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2000 reviews and 39659 distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39659)\n",
      "(1, 39659)\n",
      "(1, 39659)\n"
     ]
    }
   ],
   "source": [
    "print(token_matrix.shape)\n",
    "print(token_matrix[0].shape)\n",
    "print(token_matrix[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unique words in particular document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "print(token_matrix[0].nnz)\n",
    "print(token_matrix[1].nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0      1      2      3      4      5      6      7      8      9      \\\n",
      "0      0      0      0      0      0      0      0      0      0     10   \n",
      "\n",
      "   ...    39649  39650  39651  39652  39653  39654  39655  39656  39657  39658  \n",
      "0  ...        0      0      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[1 rows x 39659 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(token_matrix[0].todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of words in first review: \n",
      "0    682\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"total amount of words in first review: \")\n",
    "print(pd.DataFrame(token_matrix[0].todense()).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words in first review: \n",
      "0    332\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"unique words in first review: \")\n",
    "print(pd.DataFrame(token_matrix[0].todense()).astype(bool).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81437126  0.84684685  0.84684685]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(Pipeline([('vectorizer',  CountVectorizer()), ('classifier',  LogisticRegression())]), texts, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9006239   0.91283175  0.91887383]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(Pipeline([('vectorizer',  CountVectorizer()), ('classifier',  LogisticRegression())]), texts, labels, \n",
    "                      scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "    ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = Pipeline(\n",
    "            [(\"vectorizer\", CountVectorizer()),\n",
    "            (\"classifier\", LogisticRegression())]\n",
    "        )\n",
    "\n",
    "\n",
    "clf_pipeline.fit(texts, labels)\n",
    "\n",
    "print(clf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getstate__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_estimator_type', '_final_estimator', '_fit', '_get_param_names', '_get_params', '_inverse_transform', '_pairwise', '_replace_step', '_set_params', '_transform', '_validate_names', '_validate_steps', 'classes_', 'decision_function', 'fit', 'fit_predict', 'fit_transform', 'get_params', 'inverse_transform', 'named_steps', 'predict', 'predict_log_proba', 'predict_proba', 'score', 'set_params', 'steps', 'transform']\n"
     ]
    }
   ],
   "source": [
    "print(dir(clf_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]\n"
     ]
    }
   ],
   "source": [
    "print(clf_pipeline.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf_pipeline.steps[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_feature_names()   list of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_dog', '_don', '_double_team_', '_dragon', '_dragon_', '_dragonheart_', '_election', '_election_', '_entertainment_weekly_', '_escape', '_eve', '_everybody_', '_exactly_', '_experience_', '_fantastic_', '_fear_and_loathing_in_las_vegas_', '_ferris', '_fifty_', '_film', '_fisherman', '_flirting', '_four_', '_full_house_', '_gag', '_gattaca_', '_genius_', '_ghost', '_great_', '_h20_', '_halloween', '_halloween_', '_happen_', '_hard_ware', '_have_', '_heathers_', '_here_', '_highly_', '_his_', '_holy_man_', '_home', '_home_alone_', '_hope', '_huge_', '_hustler_', '_i_know', '_i_know_what_you_did_last_summer_', '_in', '_into_', '_is_', '_it', '_itcom_', '_jerry_maguire_', '_john', '_juliet_', '_jumanji_', '_kingpin_', '_knock_off_', '_la', '_last', '_last_', '_least_', '_leave', '_life', '_little', '_loathe_', '_lone', '_long_', '_looks_', '_lot_', '_mafia_', '_many_', '_matewan_', '_matrix_', '_melvin', '_mind', '_moby', '_monster_movie_', '_more_', '_mortal', '_murder_', '_must_', '_never', '_no', '_not_', '_october', '_offscreen_', '_onegin_', '_original_', '_patlabor', '_patlabor_', '_pecker_', '_people_', '_pick_chucky_up_', '_polish_wedding_', '_pollock_', '_poltergeist_', '_porky', '_practical', '_quite_', '_reach_the_rock_']\n"
     ]
    }
   ],
   "source": [
    "'''token_counts = CountVectorizer()\n",
    "   token_matrix = token_counts.fit_transform(texts)'''\n",
    "print(token_counts.get_feature_names()[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes: vocabulary_ : dict  A mapping of word terms to feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 "
     ]
    }
   ],
   "source": [
    "for k in token_counts.get_feature_names()[500:600]:\n",
    "    print(token_counts.vocabulary_[k], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39659\n",
      "39659\n"
     ]
    }
   ],
   "source": [
    "print(len(token_counts.vocabulary_))\n",
    "print(len(token_counts.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "labe = clf_pipeline.classes_\n",
    "print(labe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coefficients of each feature-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.13520383e-02  -1.78937733e-02   2.51637591e-06 ...,  -7.15499280e-03\n",
      "    3.79017415e-04  -1.40853503e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(clf_pipeline.steps[1][1].coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif = LogisticRegression()\n",
    "classif.fit(token_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.13520754e-02  -1.78938956e-02   2.51662493e-06 ...,  -7.15500203e-03\n",
      "    3.79014681e-04  -1.40853524e-03]]\n",
      "39659\n"
     ]
    }
   ],
   "source": [
    "print(classif.coef_)\n",
    "print(len(classif.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78217645403172498, -0.63661849991231045, -0.59290163936356732, -0.50817829539987214, -0.50398916836895136]\n",
      "\n",
      "[0.3688597460690598, 0.37276645512359718, 0.42673638255383145, 0.44428936989341677, 0.55606641722034611]\n"
     ]
    }
   ],
   "source": [
    "sorted_coeff = sorted(classif.coef_[0] )\n",
    "print(sorted_coeff[0:5])\n",
    "print \n",
    "print(sorted_coeff[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find the most important words for positive and negative class - words with absolute biggest values abs(n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.782176454032 bad\n",
      "0 -0.636618499912 unfortunately\n",
      "0 -0.592901639364 worst\n",
      "0 -0.5081782954 waste\n",
      "0 -0.503989168369 nothing\n",
      "0 -0.466534324306 script\n",
      "0 -0.465216836487 awful\n",
      "0 -0.463191155945 boring\n",
      "0 -0.459957900791 only\n",
      "0 -0.4426887744 plot\n",
      "\n",
      "1 0.55606641722 fun\n",
      "1 0.444289369893 great\n",
      "1 0.426736382554 back\n",
      "1 0.372766455124 quite\n",
      "1 0.368859746069 well\n",
      "1 0.363306047217 seen\n",
      "1 0.358977643242 excellent\n",
      "1 0.349140946894 perfectly\n",
      "1 0.343643616257 memorable\n",
      "1 0.340132128851 overall\n"
     ]
    }
   ],
   "source": [
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print class_labels[0], coef, feat\n",
    "\n",
    "    print\n",
    "\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print class_labels[1], coef, feat\n",
    "\n",
    "\n",
    "most_informative_feature_for_binary_classification(token_counts, classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2954\n"
     ]
    }
   ],
   "source": [
    "print token_counts.vocabulary_[u'bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### supplementary material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### article from habr site: https://habr.com/post/264339/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "2257\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print(len(twenty_train.data))\n",
    "print(len(twenty_train.filenames))\n",
    "print((\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])))\n",
    "print((twenty_train.target_names[twenty_train.target[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_.get(u'algorithm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "clf_ = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf_.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834886817577\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()), ])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print(np.mean(predicted == twenty_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912782956059\n"
     ]
    }
   ],
   "source": [
    "text_clf_1 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "_ = text_clf_1.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf_1.predict(docs_test)\n",
    "print(np.mean(predicted == twenty_test.target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3),}\n",
    "gs_clf = GridSearchCV(text_clf_1, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enot\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:3: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n",
    "print(twenty_train.target_names[gs_clf.predict(['God is love'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.01\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
